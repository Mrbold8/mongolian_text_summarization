{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJ22uZdqdCmQZuWmss0VKX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrbold8/mongolian_text_summarization/blob/main/text_summarization_cp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Setup\n",
        "```"
      ],
      "metadata": {
        "id": "G3bGTIixQqE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi # runtime T4 GPU ашиглана\n",
        "\n",
        "# --- Гол libraries суулгах\n",
        "!pip -q install -U \"transformers[torch]\" datasets accelerate peft sentencepiece evaluate rouge_score bitsandbytes wandb\n",
        "\n",
        "# --- Import test хийх, version -г хэвлэх\n",
        "import torch, transformers, datasets, peft, evaluate\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"Datasets:\", datasets.__version__)\n",
        "print(\"PEFT:\", peft.__version__)"
      ],
      "metadata": {
        "id": "N7UKfJ_fQ3yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hugginface -руу нэтврэх\n",
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "H9z2YRo2XwkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocess хийхийн өмнө schema -г бататгах\n",
        "Load & inspect dataset"
      ],
      "metadata": {
        "id": "ef6tpFjvYdBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 1) Load all splits\n",
        "ds = load_dataset(\"amaraaa/mn_translated_cnn\")\n",
        "print(ds)\n",
        "\n",
        "# 2) Train split -ээс хэдэн мөрүүдийг шалгая\n",
        "for i in range(2):\n",
        "  row = ds[\"train\"][i]\n",
        "  print(f\"nRow {i} keys:\", row.keys())\n",
        "  print(\"id:\", row[\"id\"])\n",
        "  print(\"article snippet:\", (row[\"article\"] or \"\")[:200].replace(\"\\n\",\" \"))\n",
        "  print(\"highlights (type/len):\", type(row[\"highlights\"]).__name__, len(row[\"highlights\"]))\n",
        "  print(\"first highlight:\", row[\"highlights\"][0] if row[\"highlights\"] else \"-\")\n",
        "\n",
        "# 3) Sanity checks - Алдаанаас сэргийлж шалгалт хийх (data sanity checks)\n",
        "def summarize_split(name):\n",
        "  d = ds[name]\n",
        "  # Хоосон article байгаа эсэхийг шалгах\n",
        "  empty_articles = sum(1 for r in d if not r[\"article\"] or not r[\"article\"].strip())\n",
        "  # Хоосон highlights байгаа эсэхийг шалгах\n",
        "  empty_highlights = sum(1 for r in d if not r\"highlights\" or not r[\"highlights\".strip()])\n",
        "  return {\n",
        "      \"rows\": len(d),\n",
        "      \"empty_articles\": empty_articles,\n",
        "      \"empty_highlights\": empty_highlights,\n",
        "  }\n",
        "\n",
        "print(\"\\nSanity:\")\n",
        "for split in ds.keys():\n",
        "  print(split, summarize_split(split))\n"
      ],
      "metadata": {
        "id": "JGnlPg73YWsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build preprocessing - (model training -д зориулж өгөгдлийг бэлдэх)\n",
        "* mT5 нь single target string хүлээж авдаг\n",
        "* highlights -д мөр бүрт string list байгаа\n",
        "\n",
        "1) Иймээс list -ийн агуулгыг newline -тай нэг text болгоно\n",
        "2) inputs/targets -ийг tokenize хийнэ (consistent max lengths)"
      ],
      "metadata": {
        "id": "DveCvdg-ktRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- Тохируулга хийх\n",
        "model_name = \"google/mt5-small\"\n",
        "task_prefix = \"summarize: \" # t5 model -д ямар таск хийхийг prefix зааж өгнө\n",
        "max_source_length = 768\n",
        "max_target_length = 128\n",
        "\n",
        "# 1) Highlight -уудын list -ийг single string болгож join хийх\n",
        "def join_highlights(example): # (example -> single record)\n",
        "  bullets = example.get(\"highlights\") or []\n",
        "  # keep only non-empty strings and strip whitespace\n",
        "  bullets = [str(x).strip() for x in bullets if isinstance(x, str) and str(x).strip()]\n",
        "  example[\"highlights_str\"] = \"\\n\".join(bullets)\n",
        "  # print(example)\n",
        "  return example\n",
        "\n",
        "# Функцийг split бүрийн record дээр ажилуулна\n",
        "ds = ds.map(join_highlights, desc=\"Joining highlight bullets\")\n",
        "\n",
        "# 2) Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 3) Tokenize inputs & targets - (article, highlights)\n",
        "def preprocess_batch(batch):\n",
        "  # input text + task hint\n",
        "  inputs = [task_prefix + (article or \"\") for article in batch[\"article\"]]\n",
        "  targets = batch[\"highlights_str\"]\n",
        "\n",
        "  # tokenize sources\n",
        "  model_inputs = tokenizer(\n",
        "      inputs,\n",
        "      max_length=max_source_length,\n",
        "      truncation=True,\n",
        "  )\n",
        "\n",
        "  # tokenize targets (labels)\n",
        "  labels = tokenizer(\n",
        "      text_target=targets,\n",
        "      max_length=max_target_length,\n",
        "      truncation=True,\n",
        "  )\n",
        "\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "  return model_inputs\n",
        "\n",
        "# cols_to_remove = list(set(ds[\"train\"].column_names) - {\"id\"})\n",
        "# Remove original columns and the joined highlights string, including 'id'\n",
        "cols_to_remove = [\"id\", \"article\", \"highlights\", \"highlights_str\"]\n",
        "\n",
        "tokenized_ds = ds.map(\n",
        "    preprocess_batch,\n",
        "    batched=True,\n",
        "    remove_columns=cols_to_remove,\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "\n",
        "print(tokenized_ds)\n",
        "print(\"Example ids:\", tokenized_ds[\"train\"][0][\"input_ids\"][:20], \"...\")\n",
        "print(\"Example labels:\", tokenized_ds[\"train\"][0][\"labels\"][:20], \"...\")\n",
        "\n",
        "# --- Tokenized хослолыг шалгах\n",
        "sample = tokenized_ds[\"train\"][0]\n",
        "print(tokenizer.decode(sample[\"input_ids\"][:128], skip_special_tokens=True))\n",
        "\n",
        "# Safe label preview: (replace -100 with pad id before decoding)\n",
        "safe_labels = [(tid if tid != -100 else tokenizer.pad_token_id) for tid in sample[\"labels\"]]\n",
        "print(tokenizer.decode(safe_labels[:64], skip_special_tokens=True))\n",
        "\n"
      ],
      "metadata": {
        "id": "xhcbmuLenKTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load mT5-small & attach LoRa adapters\n",
        "LoRa гэх мэт PEFT method -оор model сургахдаа бэлдэх (peft configuration)\n",
        "\n",
        "LoRa -> суурь model -ийг бүхэлд нь биш зөвхөн тодорхой цөөн weight -үүдийг сургана.\n",
        "*   Resrource бага шаардана\n",
        "*   Илүү efficient байх боломжтой\n",
        "*   Model -ийн цөөн тооны parameter -ийг үр дүнтэй fine-tune хийнэ.\n",
        "*   Computational болон storage cost багасгана.\n"
      ],
      "metadata": {
        "id": "moasMESecc12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "#1) Load base model\n",
        "base_model_name = \"google/mt5-small\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
        "\n",
        "#2) (recommended for memory) - enable gradient checkpointing later during training\n",
        "model.config.use_cache = False\n",
        "# model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
        "\n",
        "#3) LoRa config тохируулах\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=16, # rank (capacity of adapters)\n",
        "    lora_alpha=32, #scaling\n",
        "    lora_dropout=0.1, # regularization\n",
        "    target_modules=[\"q\", \"v\"], # T5 attention projections commonly adapted\n",
        ")\n",
        "\n",
        "#4) Wrap model with LoRa - (mt5 -д loRa technique -ийг apply хийнэ)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Enable input gradients for gradient checkpointing\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# Move the model to the GPU\n",
        "model.to(\"cuda\")\n",
        "\n",
        "\n",
        "#5) Quick sanity: хэчнээн params сургахаа харах\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "f4-UXClecgS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Sentinel blocklist үүсгэх (T5 extra_id 0..99) -> for generation\n",
        "vsize = tokenizer.vocab_size\n",
        "extra_ids = list(range(vsize - 100, vsize))\n",
        "bad_words_ids = [[i] for i in extra_ids]\n",
        "print(f\"Blocking {len(bad_words_ids)} sentinel IDs: {extra_ids[0]}..{extra_ids[-1]}\")"
      ],
      "metadata": {
        "id": "mgqoPmGuyOKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer тохируулга (metrics, collator, hyperparams)\n",
        "Rouge metrics, batching/padding, hyperparameters тохируулах\n"
      ],
      "metadata": {
        "id": "Q89eN-oXcs68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import(\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "\n",
        "#1) Metric: ROUGE\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "  # strip хийгээд sentence хоороod newline нэмэх\n",
        "  preds = [p.strip() for p in preds]\n",
        "  labels = [l.strip() for l in labels]\n",
        "  return preds, labels\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  preds, labels = eval_pred\n",
        "\n",
        "  # Some trainers return (sequences)\n",
        "  if isinstance(preds, tuple):\n",
        "      preds = preds[0] #take token IDs matrix\n",
        "\n",
        "  # IMPORTANT: decode хийхээс өмнө ignore index (-100) -г real pad token ID -аар солих\n",
        "  #preds\n",
        "  if isinstance(preds, np.ndarray):\n",
        "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
        "  else:\n",
        "    # fallback in case preds is a list\n",
        "    preds = [[(tok if tok != -100 else tokenizer.pad_token_id) for tok in seq] for seq in preds]\n",
        "  #labels\n",
        "  if isinstance(labels, np.ndarray):\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  else:\n",
        "    # fallback in case labels is a list\n",
        "    labels = [[(tok if tok != -100 else tokenizer.pad_token_id) for tok in seq] for seq in labels]\n",
        "\n",
        "  # print(\"Debugging: (computer_metrics) \")\n",
        "  # print(\"Type of preds:\", type(preds))\n",
        "  # print(\"Preds -> first 50 values\", preds[:50])\n",
        "  # print(\"---------------------------------\")\n",
        "\n",
        "  # decode predictions\n",
        "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "  print(decoded_preds)\n",
        "\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # light cleanup for ROUGE\n",
        "  decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "  # Compute ROUGE\n",
        "  result = rouge.compute(\n",
        "      predictions=decoded_preds,\n",
        "      references=decoded_labels,\n",
        "      use_stemmer=True,\n",
        "  )\n",
        "\n",
        "  # # add a smiple generation length metric\n",
        "  # gen_lens = [np.count_nonzero(p != tokenizer.pad_token_id) for p in preds]\n",
        "  # result[\"gen_len\"] = float(np.mean(gen_lens))\n",
        "  # # focus metric alias\n",
        "  # result[\"rougeLsum\"] = result.get(\"rougeLsum\", result.get(\"rougeL\", 0.0))\n",
        "  # return {k: round(v * 100, 4) if k.startswith(\"rouge\") else round(v, 2) for k, v in result.items()}\n",
        "\n",
        "  return {k: round(v * 100, 4) for k, v in result.items()}\n",
        "\n",
        "#2) dynamic padding & label masking (-100)\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
        ")\n",
        "\n",
        "#3) Enable memory savers\n",
        "model.gradient_checkpointing_enable() # reduces VRAM\n",
        "\n",
        "#4) Training arguments (tuned for small data + LoRA)\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"mt5_mncnn_lora\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-4,\n",
        "    num_train_epochs=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=128,\n",
        "    generation_num_beams=4,\n",
        "    warmup_ratio=0.03,\n",
        "    # lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_rougeLsum\",\n",
        "    greater_is_better=True,\n",
        "    fp16=False,  #AMP on Colab GPUs\n",
        "    # label_smoothing_factor=0.1,\n",
        "    # max_grad_norm=0.5,\n",
        "    # remove_unused_columns=True,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # Decode hygiene\n",
        "    # no_repeat_ngram_size=3,\n",
        "    # length_penalty=1.0,\n",
        "    # early_stopping=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Add early stopping (prevents overfitting on 369 rows)\n",
        "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
        "\n",
        "print(\"Ready to train.\")\n",
        "print(\"Train rows:\", len(tokenized_ds[\"train\"]), \"| Val rows:\", len(tokenized_ds[\"validation\"]))"
      ],
      "metadata": {
        "id": "cANT7RSCcv3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sanity check before full training"
      ],
      "metadata": {
        "id": "xnuyrRcIozvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Small slice of eval to test metrics & decoding path\n",
        "small_eval = tokenized_ds[\"validation\"].select(range(20))\n",
        "eval_result = trainer.evaluate(eval_dataset=small_eval)\n",
        "\n",
        "print(eval_result)\n"
      ],
      "metadata": {
        "id": "YLijxRr_o2T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_s7VTZp723Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Small slice of eval to test metrics & decoding path\n",
        "small_eval = tokenized_ds[\"train\"].select(range(20))\n",
        "eval_result = trainer.evaluate(eval_dataset=small_eval)\n",
        "\n",
        "print(eval_result)\n"
      ],
      "metadata": {
        "id": "KB6RT98424wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training эхлүүлэх - (LoRa on mT5-small)"
      ],
      "metadata": {
        "id": "LuvqgYISdWwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ──────────────────────────────────────────────────────────────\n",
        "# # 8) Evaluate → Train → Evaluate\n",
        "# # ──────────────────────────────────────────────────────────────\n",
        "print(\"Eval (before training):\")\n",
        "metrics_before = trainer.evaluate()\n",
        "print(metrics_before)\n",
        "\n",
        "# Training\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Save the best checkpoint (load_best_model_at_the_end=True)\n",
        "trainer.save_model(\"mt5_mncnn_lora/best\") # saves LoRa adapter weights + config\n",
        "tokenizer.save_pretrained(\"mt5_mncnn_lora/best\")\n",
        "\n",
        "# see a short summary of training\n",
        "print(train_result)\n",
        "\n",
        "# Eval after training\n",
        "print(\"Eval (before training):\")\n",
        "metrics_after = trainer.evaluate()\n",
        "print(metrics_after)"
      ],
      "metadata": {
        "id": "nybotzVHdXoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lock the best checkpoint, evaluate on test & inspect generations (with sentinel-blocking)\n",
        "\n",
        "*   Best checkpoint -> хадгалах\n",
        "*   Test dataset дээр evaluate хийх\n",
        "*   Generations -ийг шалгах (block **sentinel tokens**)\n"
      ],
      "metadata": {
        "id": "XjdZRpXVd4IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Best checkpoint -ийг хадгалах\n",
        "import torch, numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from peft import PeftModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_ckpt = getattr(trainer.state, \"best_model_checkpoint\", None)\n",
        "print(\"Best checkpoint:\", best_ckpt)\n",
        "\n",
        "# If None, fall back to trainer.model as-is; else reload clean\n",
        "if best_ckpt:\n",
        "  base_eval = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")\n",
        "  model_eval = PeftModel.from_pretrained(base_eval, best_ckpt).to(device)\n",
        "else:\n",
        "  model_eval = trainer.model.to(device)\n",
        "\n",
        "model_eval.eval()\n",
        "\n",
        "# 2) Sentinel blocklist үүсгэх (T5 extra_id 0..99) -> for generation\n",
        "vsize = tokenizer.vocab_size\n",
        "extra_ids = list(range(vsize - 100, vsize))\n",
        "bad_words_ids = [[i] for i in extra_ids]\n",
        "print(f\"Blocking {len(bad_words_ids)} sentinel IDs: {extra_ids[0]}..{extra_ids[-1]}\")\n",
        "\n",
        "# 3) DataLoader for test split\n",
        "test_loader = DataLoader(\n",
        "    tokenized_ds[\"test\"],\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "# 4) Generate with constraints & compute ROUGE on the test set\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "all_preds, all_refs = [], []\n",
        "\n",
        "for batch in test_loader:\n",
        "  labels = batch[\"labels\"].numpy()\n",
        "  refs = tokenizer.batch_decode(\n",
        "    np.where(labels != -100, labels, tokenizer.pad_token_id),\n",
        "    skip_special_tokens=True,\n",
        "  )\n",
        "  with torch.no_grad():\n",
        "    gen = model_eval.generate(\n",
        "        input_ids=batch[\"input_ids\"].to(device),\n",
        "        attention_mask=batch[\"attention_mask\"].to(device),\n",
        "        num_beams=4,\n",
        "        max_new_tokens=128,\n",
        "        no_repeat_ngram_size=3, #reduce loops\n",
        "        repetition_penalty=1.15,\n",
        "        bad_words_ids=bad_words_ids, #block sentinel tokens\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "  preds = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "  all_refs.extend([r.strip() for r in refs])\n",
        "  all_preds.extend([p.strip() for p in preds])\n",
        "\n",
        "test_metrics = rouge.compute(\n",
        "    predictions=all_preds,\n",
        "    references=all_refs,\n",
        "    use_stemmer=False,\n",
        ")\n",
        "test_metrics = {k: round(v * 100, 2) for k, v in test_metrics.items()}\n",
        "print(\"TEST ROUGE:\", test_metrics)\n",
        "\n",
        "# 5) Show a few qualitative examples\n",
        "def show_pairs(n=5):\n",
        "  for i in range(n):\n",
        "    print(f\"\\n--- Example{i+1} ---\")\n",
        "    print(\"Pred:\", all_preds[i][:400])\n",
        "    print(\"Ref:\", all_refs[i][:400])\n",
        "\n",
        "show_pairs(5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wJ-0ArMOd253"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сургасан model -оо хадгалах\n",
        "\n",
        "#1) Google Drive -руу хадгалах\n",
        "SAVE_DIR = \"/content/drive/MyDrive/mt5_mncnn_lora_best_v1\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "trainer.save_model(SAVE_DIR) # with PEFT: saves LoRA adapter + peft_config.json\n",
        "tokenizer.save_pretrained(SAVE_DIR) # tokenizer files\n",
        "model.config.save_pretrained(SAVE_DIR)\n",
        "print(\"Saved to:\", SAVE_DIR)\n"
      ],
      "metadata": {
        "id": "kVgYPjR5nfPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Сургасан model -оо ачааллах.\n",
        "!pip -q install transformers peft accelerate sentencepiece\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from peft import PeftModel\n",
        "import torch, numpy as np\n",
        "\n",
        "ADAPTER_DIR = \"/content/drive/MyDrive/mt5_mncnn_lora_best_v1\"\n",
        "BASE = \"google/mt5-small\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR)\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(BASE, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None)\n",
        "model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
        "\n",
        "# Safety: pad/decoder ids\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token or \"<pad>\"\n",
        "pad_id = tokenizer.pad_token_id\n",
        "model.config.pad_token_id = pad_id\n",
        "if getattr(model.config, \"decoder_start_token_id\", None) is None:\n",
        "    model.config.decoder_start_token_id = pad_id\n",
        "\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "bMrxpWZznW5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input оруулж model -ийн үр дүн хураангуйг шалгах"
      ],
      "metadata": {
        "id": "BP0F8vluiGRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Single-input inference helper ===\n",
        "# import torch, numpy as np\n",
        "# from transformers import AutoModelForSeq2SeqLM\n",
        "# from peft import PeftModel\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "text = \"Сүүлийн өдрүүдэд нийслэл болон аймгуудын шатахуун түгээх станцууд дээр автомашиндаа бензин авах гэсэн иргэдийн урт дараалал үүсч, зарим газарт шатахуун дууссанаа мэдэгдэх болсон. Иргэд машинынхаа банкийг дүүртэл шатахуун авч байгаа нь дотоодын хэрэглээний зохицуулалтад хүндрэл үүсгэж, богино хугацаанд нийлүүлэлтийн ачааллыг нэмэгдүүлж байна гэж холбогдох албаны хүмүүс тайлбарласан. Аж үйлдвэр, эрдэс баялгийн сайд Г.Дамдинням энэ талаар өөрийн “Х” хуудастаа “130 вагон АИ-92 шатахуун ОХУ-аас ачигдан манай улс руу тээвэрлэгдэж байна. Үүнээс өнөөдөр 62 вагон нь хилээр нэвтэрч ирэх хуваарьтай бөгөөд энэ нь нийтдээ 4000 гаруй тонн шатахуун гэсэн үг. Улаанбаатар хотын ердийн өдрийн хэрэглээ ойролцоогоор 1000 тонн байдаг тул ирж буй нийлүүлэлт дөрөв дахин илүү хэмжээгээр хангалт үзүүлэх боломжтой” гэжээ. Мөн тэрбээр,“Орой гэхэд нөхцөл байдал тогтворжиж, шатахуун түгээх станцуудын үйл ажиллагаа хэвийн горимд шилжинэ гэж үзэж байна” хэмээн мэдээлсэн байна.\"\n",
        "\n",
        "# 2) Build robust blocklist\n",
        "#    - T5/mT5 sentinel tokens are the last 100 vocab IDs\n",
        "vsize = tokenizer.vocab_size\n",
        "bad_words_ids = [[i] for i in range(vsize - 100, vsize)]\n",
        "\n",
        "#    - Also block the stray \"langsung\" if it tokenizes cleanly\n",
        "# bad_lang = tokenizer.encode(\"langsung\", add_special_tokens=False)\n",
        "# if isinstance(bad_lang, list) and len(bad_lang) > 0:\n",
        "#     bad_words_ids.append(bad_lang)\n",
        "\n",
        "def mn_summarize(\n",
        "    text: str,\n",
        "    num_beams: int = 4,\n",
        "    max_new_tokens: int = 128,\n",
        "    no_repeat_ngram_size: int = 4,\n",
        "    repetition_penalty: float = 1.2,\n",
        "    length_penalty: float = 1.0,\n",
        "):\n",
        "    # Prefix helps T5-style models\n",
        "    prompt = \"summarize: \" + (text or \"\")\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=768,\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen = model_eval.generate(\n",
        "            **inputs,\n",
        "            num_beams=num_beams,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            length_penalty=length_penalty,\n",
        "            bad_words_ids=bad_words_ids,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "\n",
        "# 3) Try it on a test row (or paste your own Mongolian text)\n",
        "# print(\"Summary: \")\n",
        "# print(mn_summarize(text))\n",
        "\n",
        "# Test дээр турших\n",
        "sample_text = ds[\"test\"][0][\"article\"]\n",
        "print(\"INPUT (snippet):\", sample_text[:300].replace(\"\\n\",\" \"))\n",
        "print(\"\\nSUMMARY:\", mn_summarize(sample_text))\n"
      ],
      "metadata": {
        "id": "It9353PIiOsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(ds)\n",
        "# print(ds[\"train\"])\n",
        "# print(ds[\"train\"][1][\"highlights\"])\n",
        "\n",
        "# def square(number):\n",
        "#   return number * number\n",
        "\n",
        "# number = [1, 2, 3, 4 , 5]\n",
        "# square_nums = map(square, number)\n",
        "# print(list(square_nums))"
      ],
      "metadata": {
        "id": "H2gO7qmHqBe6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}